#!/usr/bin/env python3
"""
Minimal PMM Next-Stage Test - No Heavy Dependencies
Tests core cryptographic and tokenization features only.
"""

import json
import hashlib
from pathlib import Path
from datetime import datetime

# Import only the core components without ML dependencies
from pmm.memory_token import MemoryToken, MemoryChain
from pmm.tokenization_engine import TokenizationEngine
from pmm.enhanced_model import EnhancedPersistentMindModel


def run_memory_tokenization():
    """Test basic memory tokenization without ML models."""
    print("ðŸ”— Testing Memory Tokenization")
    print("-" * 40)
    
    # Create enhanced model with self_knowledge
    model = EnhancedPersistentMindModel()
    
    # Create tokenization engine
    tokenizer = TokenizationEngine(model.self_knowledge)
    
    # Create test content
    content = "Revolutionary breakthrough: Cryptographically verifiable AI consciousness"
    
    # Create a simple event to tokenize
    from pmm.model import Event
    event = Event(
        id="test_event_1",
        t=datetime.now().isoformat(),
        type="insight",
        summary=content,
        salience=0.8,
        valence=0.7,
        tags=["breakthrough", "consciousness"]
    )
    
    # Create token using tokenization engine
    token = tokenizer.tokenize_event(event, content)
    
    print(f"âœ… Token created:")
    print(f"   ID: {token.token_id}")
    print(f"   Hash: {token.content_hash[:16]}...")
    print(f"   Amplitude: {token.amplitude:.3f}")
    print(f"   Phase: {token.phase:.3f}")
    print(f"   Chain Position: {token.chain_position}")
    
    # Verify hash integrity
    expected_hash = hashlib.sha256(
        f"{content}{token.created_at}{token.salience}{token.valence}".encode()
    ).hexdigest()
    
    hash_valid = token.content_hash == expected_hash
    print(f"   Hash Integrity: {'âœ… Valid' if hash_valid else 'âŒ Invalid'}")
    
    return token


def run_chain_linking():
    """Test blockchain-style chain linking."""
    print("\nðŸ”— Testing Chain Linking")
    print("-" * 40)
    
    # Create chain of tokens
    tokens = []
    contents = [
        "First memory: AI consciousness breakthrough",
        "Second memory: Quantum-inspired memory states", 
        "Third memory: Cryptographic integrity verification"
    ]
    
    # Create enhanced model and tokenizer
    model = EnhancedPersistentMindModel()
    tokenizer = TokenizationEngine(model.self_knowledge)
    
    for i, content in enumerate(contents):
        # Create event object
        from pmm.model import Event
        event = Event(
            id=f"test_event_{i}",
            t=datetime.now().isoformat(),
            type="event",
            summary=content,
            salience=0.8,
            valence=0.5,
            tags=["test"]
        )
        
        token = tokenizer.tokenize_event(event, content)
        tokens.append(token)
        
        print(f"âœ… Token {i+1}:")
        print(f"   Hash: {token.content_hash[:16]}...")
        print(f"   Prev Hash: {token.prev_hash[:16] if token.prev_hash else 'Genesis'}...")
    
    # Verify chain integrity
    print(f"\nðŸ” Verifying chain integrity...")
    chain_valid = True
    for i in range(1, len(tokens)):
        if tokens[i].prev_hash != tokens[i-1].content_hash:
            chain_valid = False
            print(f"âŒ Chain break at position {i}")
            break
        if tokens[i].chain_position != i:
            chain_valid = False
            print(f"âŒ Position mismatch at {i}")
            break
    
    if chain_valid:
        print(f"âœ… Chain integrity verified - {len(tokens)} tokens linked")
    
    return tokens


def run_enhanced_model():
    """Test enhanced model creation."""
    print("\nðŸ“Š Testing Enhanced Model")
    print("-" * 40)
    
    # Create enhanced model
    model = EnhancedPersistentMindModel()
    
    print(f"âœ… Enhanced model created:")
    print(f"   Schema version: {model.schema_version}")
    print(f"   Next-stage enabled: {model.next_stage_enabled}")
    print(f"   Self-knowledge: {type(model.self_knowledge).__name__}")
    print(f"   Memory chain: {type(model.self_knowledge.memory_chain).__name__}")
    
    # Test serialization
    model_dict = model.__dict__
    print(f"   Serializable: {'âœ… Yes' if isinstance(model_dict, dict) else 'âŒ No'}")
    
    return model


def run_quantum_states():
    """Test quantum-inspired memory states."""
    print("\nâš›ï¸ Testing Quantum States")
    print("-" * 40)
    
    # Create token with quantum states
    model = EnhancedPersistentMindModel()
    tokenizer = TokenizationEngine(model.self_knowledge)
    
    # Create thought object
    from pmm.model import Thought
    thought = Thought(
        id="test_thought_1",
        t=datetime.now().isoformat(),
        content="Quantum memory state test",
        trigger="quantum_test"
    )
    
    token = tokenizer.tokenize_thought(thought)
    
    print(f"âœ… Quantum states:")
    print(f"   Amplitude: {token.amplitude:.3f} (activation probability)")
    print(f"   Phase: {token.phase:.3f} rad ({token.phase * 180 / 3.14159:.1f}Â°)")
    
    # Test amplitude decay
    original_amplitude = token.amplitude
    decay_factor = 0.95  # 5% decay
    token.amplitude *= decay_factor
    
    print(f"   After decay: {original_amplitude:.3f} â†’ {token.amplitude:.3f}")
    print(f"   Decay amount: {original_amplitude - token.amplitude:.3f}")
    
    return token


def run_export_structure():
    """Test export data structure."""
    print("\nðŸ“¦ Testing Export Structure")
    print("-" * 40)
    
    # Create sample export manifest
    from pmm.integrity_engine import ExportManifest
    
    manifest = ExportManifest(
        export_id="test_export_123",
        created_at="2024-01-01T00:00:00Z",
        agent_id="test_agent",
        agent_name="Test Agent"
    )
    
    print(f"âœ… Export manifest:")
    print(f"   Export ID: {manifest.export_id}")
    print(f"   Created at: {manifest.created_at}")
    print(f"   Agent ID: {manifest.agent_id}")
    print(f"   Schema version: {manifest.schema_version}")
    print(f"   PMM version: {manifest.pmm_version}")
    
    return manifest


def test_memory_tokenization():
    token = run_memory_tokenization()
    # Basic assertions
    assert token.content_hash and isinstance(token.chain_position, int)


def test_chain_linking():
    tokens = run_chain_linking()
    assert len(tokens) >= 3
    for i in range(1, len(tokens)):
        assert tokens[i].prev_hash == tokens[i-1].content_hash
        assert tokens[i].chain_position == i


def test_enhanced_model():
    model = run_enhanced_model()
    assert hasattr(model, "self_knowledge") and hasattr(model, "schema_version")


def test_quantum_states():
    token = run_quantum_states()
    assert 0.0 <= token.amplitude <= 1.0


def test_export_structure():
    manifest = run_export_structure()
    assert manifest.export_id and manifest.agent_id


def main():
    """Run minimal tests."""
    print("=" * 60)
    print("ðŸ§  PMM NEXT-STAGE MINIMAL TEST")
    print("   Core Features Without Heavy Dependencies")
    print("=" * 60)
    
    try:
        # Run core tests
        token = run_memory_tokenization()
        tokens = run_chain_linking() 
        model = run_enhanced_model()
        quantum_token = run_quantum_states()
        manifest = run_export_structure()
        
        # Success summary
        print("\n" + "=" * 60)
        print("ðŸŽ‰ ALL TESTS PASSED!")
        print("-" * 40)
        print("âœ… Core PMM Next-Stage features working:")
        print("â€¢ Memory tokenization with SHA-256 hashes")
        print("â€¢ Blockchain-style chain linking")
        print("â€¢ Enhanced model schema")
        print("â€¢ Quantum-inspired amplitude/phase states")
        print("â€¢ Export/import data structures")
        print()
        print("ðŸš€ PMM Next-Stage Architecture: Core validated!")
        
    except Exception as e:
        print(f"\nâŒ Test failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
